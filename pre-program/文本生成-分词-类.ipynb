{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import collections\n",
    "import math\n",
    "import random\n",
    "import zipfile\n",
    "import fileUtil\n",
    "from matplotlib import pylab\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "import csv\n",
    "import jieba.posseg as jp, jieba\n",
    "\n",
    "from gensim.models import doc2vec, ldamodel\n",
    "from gensim import corpora, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "段落处理完毕\n",
      "Length of text: 100 characters\n"
     ]
    }
   ],
   "source": [
    "dir_name = 'dataset'\n",
    "file_name = 'hotel_desc_seed_remove_200line.csv'\n",
    "document = fileUtil.read_data_as_sentence_array(dir_name, file_name)\n",
    "# 文本长度是指文本中的字符个数\n",
    "print ('Length of text: {} characters'.format(len(document)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['上海时生隅园林文化酒店 2017年的秋末 时生隅初见END彼时的心愿 想要在快节奏的都市 寻一处讲诉时光和生命的角落END不问对错 不闻浮沉END这便是时生隅的由来END这一次 我们秉承初心 在闹中取静的川沙地铁站商圈 觅得一方天地 充满江南气息的园林 叶园END长廊湖景 古雅庭院END这里淡雅而大气 是快节奏与民俗文化的交融 这里幽静而明快 是远道而来亦或小憩而居的别样天地END时生隅的新老朋友 我们在叶园 期待与您共襄当下景 把时光言欢END', '云和夜泊酒店 位于浦东新区祝桥秋亭路 毗邻上海浦东国际机场 车程20分钟 距上海迪士尼18分钟车程 距上海野生动物园21分钟车程END酒店地理位置优越 并配备大巴 中巴 豪华轿车接送客户END酒店拥有房间98间 大床30间 行政大床15间 标间30间 亲子房23间END', '维也纳酒店位于浦东新区川南奉公路 近晨阳路 与迪士尼直线距离约8公里 可便捷到达地铁2号线凌空路站 交通便利END酒店周围生活设施齐全 旅游资源众多 有上海新国际博览中心 迪士尼 上海野生动物园 农艺大观园 三甲港滨海旅游区等END维也纳酒店是维也纳旗下的连锁酒店 装修豪华舒适 整体风格高贵典雅END客房宽敞明亮 温馨时尚 房内布置精美 处处体现人性化的理念END酒店还有宽敞停车场 休闲茶吧 宽敞会议室等 同时还为您提供精品早餐 浦东机场接机等服务 是商务 休闲 会务的理想酒店END', '上海万信r酒店位于浦东新区崮山路 地处内环线陆家嘴金融区内 近地铁6号线 地铁9号线 配备专属停车场 自驾至新国际博览中心约10分钟 至外滩 世博园区 南京路步行街 上海自由贸易区 上海迪斯尼乐园约20分钟END酒店拥有百余间崭新客房 精选配套 让舒适与轻奢相拥 酒店圣罗拉餐厅环境舒适安逸 主打西餐 辅以粤菜 川菜 沪菜 在此享受由港粤名厨精心烹饪的佳肴 舌尖不由自主的跳起了芭蕾END早餐的菜品琳琅满目可达40种 为保证口味 餐品均选优质食材当天加工END酒店豪华宴会厅提供一站式婚礼服务与专业的会务服务END万信厅可支持约120人课桌式会议 丽都厅适合60人以下的各类会议需求END万信r酒店设计 突破了万信酒店一贯理念 时尚年轻 秉承r文化 华丽 贴心 轻松 享受END', '酒店位于浦东新区沪南公路3655弄2号 素有小上海之称的周浦镇 沪南公路横桥路路口 周边有地铁16号线周浦东站及11号线秀沿路站 16号线可直达海洋公园 冰雪世界等游玩景点 11号线可直达迪士尼乐园 毗邻迪士尼乐园 新国际博览中心END酒店周围有迪士尼 上海野生动物园 海昌海洋世界 上海科技馆等多个游玩景点 周边更有万达广场 绿地商城 美食 娱乐均可轻松满足END酒店由美国著名设计师打造 是知名高端设计师酒店品牌END客房典雅豪华 宽敞舒适 并配以42寸飞利浦电视 三诺音响 电动窗帘 智能灯控系统等先进科技设施设备 带来尊贵体验和高端享受END酒店每个房间都有独立的企业级wifi 带给您网络的极速体验 这里的隔音系统更由清华声学所设计 睡眠环境优越 让你一夜酣眠END酒店提供浦东国际机场 迪士尼和周浦东站的定时免费班车服务END']\n"
     ]
    }
   ],
   "source": [
    "print(document[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.675 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "flags = ('n', 'f', 's', 'nr', 'ns', 'nt', 'nw', 'nz','v','vd','vn','a','ad','an','d','q','r','p','c','u','xc','PER','LOC','ORG')  # 词性\n",
    "\n",
    "words_ls = []\n",
    "for text in document:\n",
    "    temp_words = [w.word for w in jp.cut(text,use_paddle=True)]\n",
    "    if len(temp_words) > 0:\n",
    "        words_ls.extend(temp_words)\n",
    "# print('++++++++{}', words_ls[:40])\n",
    "\n",
    "# cut_words = [w.word for w in jp.cut(document,use_paddle=True)]\n",
    "# cut_words = [w.word for w in jp.cut(document,use_paddle=True) if w.flag in flags and len(w.word) > 0]\n",
    "# print(cut_words)\n",
    "# for hotelDesc in document:\n",
    "#     temp_words = [w.word for w in jp.cut(hotelDesc,use_paddle=True) if w.flag in flags and w.word not in stopWords and len(w.word) > 0]\n",
    "#     temp_speechs = [w.flag for w in jp.cut(hotelDesc,use_paddle=True) if w.flag in flags and w.word not in stopWords and len(w.word) > 0]\n",
    "# #     words = [w.word for w in jp.cut(text) if w.flag in flags and w.word not in stopwords and len(w.word) > 0]\n",
    "# #     print('---{}, {}',temp_words, len(temp_words))\n",
    "# #     print('---{}, {}',temp_speechs, len(temp_speechs))\n",
    "#     if len(temp_words) > 0:\n",
    "#         #[[句一分词列表],[句二分词列表],...[句五分词列表]]\n",
    "#         words_ls.append(temp_words)\n",
    "#         #[[句一词性列表],[句二词性列表],...[句五词性列表]]\n",
    "#         words_speech.append(temp_speechs)\n",
    "#         #五句合并的一个列表\n",
    "#         whole_words.extend(temp_words)\n",
    "# print(words_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', '!', '+', '-', '00', '000', '01', '1', '10', '100', '1088', '11', '1132', '12', '120', '1200', '12000', '1245', '13', '135', '1389', '14', '140', '15', '150', '156', '16', '17', '173', '1788', '18', '180', '1856', '19', '1930', '2', '20', '200', '2017', '21', '211', '228', '23', '24', '25', '250', '3', '30', '300', '304', '31', '3655', '37', '4', '40', '42', '45', '450', '4s店', '5', '50', '500', '5000', '55', '6', '60', '63', '65', '66', '7', '70', '700', '71', '736', '8', '800', '82', '836', '868', '89', '9', '90', '920', '930', '944', '958', '96', '98', '985', 'END', 'END2', 'END2012', 'END2015', 'END2018', 'END24', 'END5', 'END55', 'END5km', 'END720', 'END8']\n",
      "3173 unique characters\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(set(words_ls))\n",
    "print(vocab[:100])\n",
    "print ('{} unique characters'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 30324 characters\n"
     ]
    }
   ],
   "source": [
    "text2 = open(os.path.join(dir_name,file_name), 'rb').read().decode(encoding='utf-8')\n",
    "\n",
    "# 文本长度是指文本中的字符个数\n",
    "print ('Length of text: {} characters'.format(len(text2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 243 1771 1033 ...  678 2414   89]\n"
     ]
    }
   ],
   "source": [
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "text_as_int = np.array([char2idx[c] for c in words_ls])\n",
    "print(text_as_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ' '!' '+' ... '龙华' '龙柏' '龙阳路']\n"
     ]
    }
   ],
   "source": [
    "print(idx2char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 243 1771 1033 ...  678 2414   89]\n"
     ]
    }
   ],
   "source": [
    "print(text_as_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  ' ' :   0,\n",
      "  '!' :   1,\n",
      "  '+' :   2,\n",
      "  '-' :   3,\n",
      "  '00':   4,\n",
      "  '000':   5,\n",
      "  '01':   6,\n",
      "  '1' :   7,\n",
      "  '10':   8,\n",
      "  '100':   9,\n",
      "  '1088':  10,\n",
      "  '11':  11,\n",
      "  '1132':  12,\n",
      "  '12':  13,\n",
      "  '120':  14,\n",
      "  '1200':  15,\n",
      "  '12000':  16,\n",
      "  '1245':  17,\n",
      "  '13':  18,\n",
      "  '135':  19,\n",
      "  ...\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print('{')\n",
    "for char,_ in zip(char2idx, range(20)):\n",
    "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n",
    "print('  ...\\n}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "上海\n",
      "时生隅\n",
      "园林\n",
      "文化\n",
      "酒店\n"
     ]
    }
   ],
   "source": [
    "# 设定每个输入句子长度的最大值\n",
    "seq_length = 100\n",
    "examples_per_epoch = len(text2)//seq_length\n",
    "\n",
    "# 创建训练样本 / 目标\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "for i in char_dataset.take(5):\n",
    "  print(idx2char[i.numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'上海时生隅园林文化酒店 2017年的秋末 时生隅初见END彼时的心愿 想要在快节奏的都市 寻一处讲诉时光和生命的角落END不问对错 不闻浮沉END这便是时生隅的由来END这一次 我们秉承初心 在闹中取静的川沙地铁站商圈 觅得一方天地 充满江南气息的园林 叶园END长廊湖景 古雅庭院END这里淡雅而大气 是快节奏与民俗文化的交融 这里幽静而明快 是远道而来亦或小憩'\n",
      "'而居的别样天地END时生隅的新老朋友 我们在叶园 期待与您共襄当下景 把时光言欢END云和夜泊酒店 位于浦东新区祝桥秋亭路 毗邻上海浦东国际机场 车程20分钟 距上海迪士尼18分钟车程 距上海野生动物园21分钟车程END酒店地理位置优越 并配备大巴 中巴 豪华轿车接送客户END酒店拥有房间98间 大床30间 行政大床15间 标间30间 亲子房23间END维也纳酒店位于浦东新区川南'\n",
      "'奉公路 近晨阳路 与迪士尼直线距离约8公里 可便捷到达地铁2号线凌空路站 交通便利END酒店周围生活设施齐全 旅游资源众多 有上海新国际博览中心 迪士尼 上海野生动物园 农艺大观园 三甲港滨海旅游区等END维也纳酒店是维也纳旗下的连锁酒店 装修豪华舒适 整体风格高贵典雅END客房宽敞明亮 温馨时尚 房内布置精美 处处体现人性化的理念END酒店还有宽敞停车场 休闲茶吧 宽敞'\n",
      "'会议室等 同时还为您提供精品早餐 浦东机场接机等服务 是商务 休闲 会务的理想酒店END上海万信r酒店位于浦东新区崮山路 地处内环线陆家嘴金融区内 近地铁6号线 地铁9号线 配备专属停车场 自驾至新国际博览中心约10分钟 至外滩 世博园区 南京路步行街 上海自由贸易区 上海迪斯尼乐园约20分钟END酒店拥有百余间崭新客房 精选配套 让舒适与轻奢相拥 酒店圣罗拉餐厅环境'\n",
      "'舒适安逸 主打西餐 辅以粤菜 川菜 沪菜 在此享受由港粤名厨精心烹饪的佳肴 舌尖不由自主的跳起了芭蕾END早餐的菜品琳琅满目可达40种 为保证口味 餐品均选优质食材当天加工END酒店豪华宴会厅提供一站式婚礼服务与专业的会务服务END万信厅可支持约120人课桌式会议 丽都厅适合60人以下的各类会议需求END万信r酒店设计 突破了万信酒店一贯理念 时尚年轻 '\n"
     ]
    }
   ],
   "source": [
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for item in sequences.take(5):\n",
    "  print(repr(''.join(idx2char[item.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data:  '上海时生隅园林文化酒店 2017年的秋末 时生隅初见END彼时的心愿 想要在快节奏的都市 寻一处讲诉时光和生命的角落END不问对错 不闻浮沉END这便是时生隅的由来END这一次 我们秉承初心 在闹中取静的川沙地铁站商圈 觅得一方天地 充满江南气息的园林 叶园END长廊湖景 古雅庭院END这里淡雅而大气 是快节奏与民俗文化的交融 这里幽静而明快 是远道而来亦或'\n",
      "Target data: '时生隅园林文化酒店 2017年的秋末 时生隅初见END彼时的心愿 想要在快节奏的都市 寻一处讲诉时光和生命的角落END不问对错 不闻浮沉END这便是时生隅的由来END这一次 我们秉承初心 在闹中取静的川沙地铁站商圈 觅得一方天地 充满江南气息的园林 叶园END长廊湖景 古雅庭院END这里淡雅而大气 是快节奏与民俗文化的交融 这里幽静而明快 是远道而来亦或小憩'\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in  dataset.take(1):\n",
    "  print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
    "  print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step    0\n",
      "  input: 243 ('上海')\n",
      "  expected output: 1771 ('时生隅')\n",
      "Step    1\n",
      "  input: 1771 ('时生隅')\n",
      "  expected output: 1033 ('园林')\n",
      "Step    2\n",
      "  input: 1033 ('园林')\n",
      "  expected output: 1697 ('文化')\n",
      "Step    3\n",
      "  input: 1697 ('文化')\n",
      "  expected output: 2918 ('酒店')\n",
      "Step    4\n",
      "  input: 2918 ('酒店')\n",
      "  expected output: 0 (' ')\n"
     ]
    }
   ],
   "source": [
    "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
    "    print(\"Step {:4d}\".format(i))\n",
    "    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
    "    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((128, 100), (128, 100)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 批大小\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "# 设定缓冲区大小，以重新排列数据集\n",
    "# （TF 数据被设计为可以处理可能是无限的序列，\n",
    "# 所以它不会试图在内存中重新排列整个序列。相反，\n",
    "# 它维持一个缓冲区，在缓冲区重新排列元素。） \n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 词集的长度\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# 嵌入的维度\n",
    "embedding_dim = 256\n",
    "\n",
    "# RNN 的单元数量\n",
    "rnn_units = 1024\n",
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "  model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, batch_input_shape=[batch_size, None]),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(rnn_units, return_sequences=True)),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(rnn_units, return_sequences=True)),\n",
    "    tf.keras.layers.Dense(vocab_size, activation='softmax')\n",
    "  ])\n",
    "    \n",
    "    \n",
    "#   model = tf.keras.Sequential([\n",
    "#     tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "#                               batch_input_shape=[batch_size, None]),\n",
    "#     tf.keras.layers.GRU(rnn_units,\n",
    "#                         return_sequences=True,\n",
    "#                         stateful=True,\n",
    "#                         recurrent_initializer='glorot_uniform'),\n",
    "#     tf.keras.layers.Dense(vocab_size)\n",
    "#   ])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(\n",
    "  vocab_size = len(vocab),\n",
    "  embedding_dim=embedding_dim,\n",
    "  rnn_units=rnn_units,\n",
    "  batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 100, 3173) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "  example_batch_predictions = model(input_example_batch)\n",
    "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (128, None, 256)          812288    \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (128, None, 2048)         10493952  \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (128, None, 2048)         25174016  \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (128, None, 3173)         6501477   \n",
      "=================================================================\n",
      "Total params: 42,981,733\n",
      "Trainable params: 42,981,733\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time,datetime\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
    "# 检查点保存至的目录\n",
    "checkpoint_dir = './training_checkpoints_jieba'\n",
    "\n",
    "# 检查点的文件名\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_\"+str(time.time()))\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True,\n",
    "    save_freq='epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 1 steps\n"
     ]
    }
   ],
   "source": [
    "EPOCHS=1\n",
    "# model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# latest = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "# model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "\n",
    "# model.load_weights(latest)\n",
    "# model.build(tf.TensorShape([1, None]))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string):\n",
    "  # 评估步骤（用学习过的模型生成文本）\n",
    "\n",
    "  # 要生成的字符个数\n",
    "  num_generate = 1000\n",
    "  num_sentences = 6\n",
    "  input_str = jp.cut(start_string,use_paddle=True)\n",
    "  input_eval = []\n",
    "  for s in input_str:\n",
    "    print(char2idx[s.word])\n",
    "    input_eval.append(char2idx[s.word])\n",
    "  # 将起始字符串转换为数字（向量化）\n",
    "#   input_eval = [char2idx[s.word] for s in input_str]\n",
    "  print(input_eval)\n",
    "  input_eval = tf.expand_dims(input_eval, 0)\n",
    "  print(input_eval)\n",
    "  # 空字符串用于存储结果\n",
    "  text_generated = []\n",
    "\n",
    "  # 低温度会生成更可预测的文本\n",
    "  # 较高温度会生成更令人惊讶的文本\n",
    "  # 可以通过试验以找到最好的设定\n",
    "  temperature = 1.0\n",
    "\n",
    "  # 这里批大小为 1\n",
    "  model.reset_states()\n",
    "  i = 0\n",
    "  while True:\n",
    "    if i == num_sentences:\n",
    "      break\n",
    "    predictions = model(input_eval)\n",
    "    predictions = tf.squeeze(predictions, 0)\n",
    "    predictions = predictions / temperature\n",
    "    predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "    input_eval = tf.expand_dims([predicted_id], 0)\n",
    "    if idx2char[predicted_id] != 'END':\n",
    "      text_generated.append(idx2char[predicted_id])\n",
    "    else:\n",
    "      text_generated.append(idx2char[predicted_id])\n",
    "      i += 1\n",
    "    if idx2char[predicted_id] == '\\n':\n",
    "      print('end')\n",
    "  return (start_string + ''.join(text_generated))\n",
    "print(generate_text(model, start_string='人民广场'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
